{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ToDos:\n",
    "* command line for input, output folder path.\n",
    "* Add Logging\n",
    "* Add test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import pos_tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LocalFilesSystem(object):\n",
    "    def __init__(self, input_dir, output_dir):\n",
    "        self.input_dir = input_dir\n",
    "        self.output_dir = output_dir\n",
    "     \n",
    "    def load_file(self, file_name):\n",
    "        try:    \n",
    "            with open(file_name, 'r') as f:\n",
    "                data = f.readlines()\n",
    "            return data\n",
    "        except Exception as e:\n",
    "            return None\n",
    "\n",
    "    def save_file(self, data, file_name):\n",
    "        try:    \n",
    "            final_file_name = os.path.join(self.output_dir, file_name)\n",
    "            with open(final_file_name, \"w\") as f:\n",
    "                f.write(data)\n",
    "        except Exception as e:\n",
    "            return None\n",
    "               \n",
    "    def list_files_in_input(self):\n",
    "        try:\n",
    "            all_file_paths = []\n",
    "            for each_file in os.listdir(self.input_dir):\n",
    "                file_path = os.path.join(self.input_dir, each_file)\n",
    "                all_file_paths.append(file_path)\n",
    "            return all_file_paths\n",
    "        except Exception as e:\n",
    "            return []\n",
    "            \n",
    "class PreProcessing(object):\n",
    "    def __init__(self):\n",
    "        self.wordnet_lemmatizer = WordNetLemmatizer()\n",
    "        self.tag_only = 'NN'\n",
    "        self.pattern_2_save = re.compile('[^a-zA-Z]')\n",
    "        self.stop_words = set(stopwords.words('english'))\n",
    "\n",
    "    def convert_list_2_str(self, token_list):\n",
    "        return '\\n'.join(token_list)\n",
    "\n",
    "    def preprocessing_folder(self, l):\n",
    "        count = 1\n",
    "        all_files_in_folder = l.list_files_in_input()\n",
    "        for each_file in all_files_in_folder:\n",
    "            data = l.load_file(each_file)\n",
    "            filtered_tokens = self.get_filtered_token_list(data)\n",
    "            output_file_name = str(count) + \".txt\"\n",
    "            data_str = self.convert_list_2_str(filtered_tokens)\n",
    "            l.save_file(data_str, output_file_name)\n",
    "            count += 1\n",
    "\n",
    "    def get_filtered_token_list(self, data):\n",
    "        all_word_tokens = []\n",
    "        filtered_tokens = []\n",
    "        for each_para in data:\n",
    "            sent_tokenize_list = sent_tokenize(each_para)\n",
    "            for each_line in sent_tokenize_list:\n",
    "                word_token_list = word_tokenize(each_line)\n",
    "                pos_tagged_list = pos_tag(word_token_list)\n",
    "                only_noun_form = [\n",
    "                    tagged[0] for tagged in pos_tagged_list if tagged[1].startswith(self.tag_only)]\n",
    "                all_word_tokens.extend(only_noun_form)\n",
    "\n",
    "        for word in all_word_tokens:\n",
    "            if not word.isalnum():\n",
    "                continue\n",
    "            word = self.pattern_2_save.sub('', word).lower()\n",
    "            if word not in self.stop_words:\n",
    "                word = self.wordnet_lemmatizer.lemmatize(word)\n",
    "                filtered_tokens.append(word)\n",
    "        return filtered_tokens\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    l = LocalFilesSystem(\"input_test\", \"output_test\")\n",
    "    p = PreProcessing()\n",
    "    p.preprocessing_folder(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
